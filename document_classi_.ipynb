{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document_classi .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Y9Qz7Y2cRKHESoUnenxS1ZwqUi3-OAmr",
      "authorship_tag": "ABX9TyOF+DfrkmXNCzbKw8bpLmHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartr99/harsha-first-course/blob/master/document_classi_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y \n",
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xySfvLLsscV",
        "outputId": "e42b18dc-e561-44ed-f685-616fdb04c2c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Found existing installation: tensorflow 1.15.0\n",
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting tensorflow==1.15\n",
            "  Using cached tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.13.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorflow-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXDoTvx_VqCm",
        "outputId": "a0821ddd-f195-4f63-9e3c-fcf7c1fbcce1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-14 16:09:45--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37922 (37K) [text/plain]\n",
            "Saving to: ‘modeling.py.1’\n",
            "\n",
            "\rmodeling.py.1         0%[                    ]       0  --.-KB/s               \rmodeling.py.1       100%[===================>]  37.03K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-12-14 16:09:45 (33.7 MB/s) - ‘modeling.py.1’ saved [37922/37922]\n",
            "\n",
            "--2021-12-14 16:09:45--  https://raw.githubusercontent.com/google-research/bert/master/optimization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6258 (6.1K) [text/plain]\n",
            "Saving to: ‘optimization.py.1’\n",
            "\n",
            "optimization.py.1   100%[===================>]   6.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-14 16:09:45 (50.3 MB/s) - ‘optimization.py.1’ saved [6258/6258]\n",
            "\n",
            "--2021-12-14 16:09:45--  https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34783 (34K) [text/plain]\n",
            "Saving to: ‘run_classifier.py.1’\n",
            "\n",
            "run_classifier.py.1 100%[===================>]  33.97K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-12-14 16:09:45 (36.4 MB/s) - ‘run_classifier.py.1’ saved [34783/34783]\n",
            "\n",
            "--2021-12-14 16:09:45--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12257 (12K) [text/plain]\n",
            "Saving to: ‘tokenization.py.1’\n",
            "\n",
            "tokenization.py.1   100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-14 16:09:46 (83.9 MB/s) - ‘tokenization.py.1’ saved [12257/12257]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import re, os\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "\n",
        "# BERT\n",
        "import optimization \n",
        "import run_classifier\n",
        "import tokenization\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjowr7D2Vuht",
        "outputId": "709779ae-7834-415c-9424-15a38e8a94b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the file details\n",
        "directory = []\n",
        "file = []\n",
        "title = []\n",
        "text = []\n",
        "label = []\n",
        "datapath = '/content/drive/MyDrive/datasets/document'\n",
        "for dirname, _ , filenames in os.walk(datapath):\n",
        "    #print('Directory: ', dirname)\n",
        "    #print('Subdir: ', dirname.split('/')[-1])\n",
        "    # remove the Readme.txt file\n",
        "    # will not find file in the second iteration so we skip the error\n",
        "    try:\n",
        "        filenames.remove('README.TXT')\n",
        "    except:\n",
        "        pass\n",
        "    for filename in filenames:\n",
        "        directory.append(dirname)\n",
        "        file.append(filename)\n",
        "        label.append(dirname.split('/')[-1])\n",
        "        fullpathfile = os.path.join(dirname,filename)\n",
        "        with open(fullpathfile, 'r', encoding=\"utf8\", errors='ignore') as infile:\n",
        "            intext = ''\n",
        "            firstline = True\n",
        "            for line in infile:\n",
        "                if firstline:\n",
        "                    title.append(line.replace('\\n',''))\n",
        "                    firstline = False\n",
        "                else:\n",
        "                    intext = intext + ' ' + line.replace('\\n','')\n",
        "            text.append(intext)"
      ],
      "metadata": {
        "id": "wme-Zxkfu8VG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "\n",
        "fulldf = pd.DataFrame(list(zip(directory, file, title, text, label)), \n",
        "               columns =['directory', 'file', 'title', 'text', 'label'])\n",
        "\n",
        "df = fulldf.filter(['text','label'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OC_utP-SvK_I",
        "outputId": "51b678d9-91e6-40a2-9220-280f6180495c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>International films will be given the same p...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A TV station in the US has refused to show a...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The sixth and final Star Wars movie may not ...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MTV has been criticised for \"incessant sleaz...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Plans to create a US soap based on the BBC's...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text          label\n",
              "0    International films will be given the same p...  entertainment\n",
              "1    A TV station in the US has refused to show a...  entertainment\n",
              "2    The sixth and final Star Wars movie may not ...  entertainment\n",
              "3    MTV has been criticised for \"incessant sleaz...  entertainment\n",
              "4    Plans to create a US soap based on the BBC's...  entertainment"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aGjVbRFvSiG",
        "outputId": "bf687aa8-f698-4fc6-9b47-3f2039489d30"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpbOkKjDvZXO",
        "outputId": "3716fee9-4f28-414f-eb7d-faa81c4f3f83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label in np.unique(df['label']):\n",
        "    print(label)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxfB9aTqvbdG",
        "outputId": "2e0f706f-b67c-4468-e68e-c795ce825390"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business\n",
            "entertainment\n",
            "politics\n",
            "sport\n",
            "technologie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking number of records of each label\n",
        "df['label'].value_counts().sort_values(ascending=False).plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "33FBkmrKvnrW",
        "outputId": "e2e73e12-3c4b-4371-c80a-27e0c78f4deb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1185fb9150>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fRZ51MPXvscl",
        "outputId": "74abf798-0de4-420e-fe7b-4534d069c349"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>International films will be given the same p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A TV station in the US has refused to show a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The sixth and final Star Wars movie may not ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MTV has been criticised for \"incessant sleaz...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Plans to create a US soap based on the BBC's...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0    International films will be given the same p...      1\n",
              "1    A TV station in the US has refused to show a...      1\n",
              "2    The sixth and final Star Wars movie may not ...      1\n",
              "3    MTV has been criticised for \"incessant sleaz...      1\n",
              "4    Plans to create a US soap based on the BBC's...      1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NroYBf90GL_",
        "outputId": "4a354f90-4b83-47b7-a5e7-dfd2ff2e2b4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "#     text = re.sub(r'\\W+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text \n",
        "    return text"
      ],
      "metadata": {
        "id": "-ZqCF2uIvx9l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(clean_text)\n",
        "df['text'] = df['text'].str.replace('\\d+', '')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KvxQpGrM0W3t",
        "outputId": "0b9b588e-ee03-4f92-9159-fe498dd08940"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>international films given prominence us films ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tv station us refused show controversial new s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sixth final star wars movie may suitable young...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mtv criticised incessant sleaze television ind...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>plans create us soap based bbcs eastenders rep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  international films given prominence us films ...      1\n",
              "1  tv station us refused show controversial new s...      1\n",
              "2  sixth final star wars movie may suitable young...      1\n",
              "3  mtv criticised incessant sleaze television ind...      1\n",
              "4  plans create us soap based bbcs eastenders rep...      1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split(text1):\n",
        "    l_total = []\n",
        "    l_parcial = []\n",
        "    if len(text1.split())//150 >0:\n",
        "        n = len(text1.split())//150\n",
        "    else: \n",
        "        n = 1\n",
        "    for w in range(n):\n",
        "        if w == 0:\n",
        "            l_parcial = text1.split()[:200]\n",
        "            l_total.append(\" \".join(l_parcial))\n",
        "        else:\n",
        "            l_parcial = text1.split()[w*150:w*150 + 200]\n",
        "            l_total.append(\" \".join(l_parcial))\n",
        "    return l_total"
      ],
      "metadata": {
        "id": "aFT7XXFU0auu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_split'] = df['text'].apply(get_split)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b7PUaTD-0dp2",
        "outputId": "21458c5e-9bcb-4401-c6fd-ba2d34b4f51b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>international films given prominence us films ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[international films given prominence us films...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tv station us refused show controversial new s...</td>\n",
              "      <td>1</td>\n",
              "      <td>[tv station us refused show controversial new ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sixth final star wars movie may suitable young...</td>\n",
              "      <td>1</td>\n",
              "      <td>[sixth final star wars movie may suitable youn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mtv criticised incessant sleaze television ind...</td>\n",
              "      <td>1</td>\n",
              "      <td>[mtv criticised incessant sleaze television in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>plans create us soap based bbcs eastenders rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>[plans create us soap based bbcs eastenders re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         text_split\n",
              "0  international films given prominence us films ...  ...  [international films given prominence us films...\n",
              "1  tv station us refused show controversial new s...  ...  [tv station us refused show controversial new ...\n",
              "2  sixth final star wars movie may suitable young...  ...  [sixth final star wars movie may suitable youn...\n",
              "3  mtv criticised incessant sleaze television ind...  ...  [mtv criticised incessant sleaze television in...\n",
              "4  plans create us soap based bbcs eastenders rep...  ...  [plans create us soap based bbcs eastenders re...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the output directory for saving model file\n",
        "OUTPUT_DIR = '/bert_news_category'\n",
        "\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "\n",
        "if DO_DELETE:\n",
        "    try:\n",
        "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "#         tf.compat.v1.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFk0Y1T70hw2",
        "outputId": "221bb82d-f32e-478c-fb23-b3de5221d77a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Model output directory: /bert_news_category *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(df, test_size=0.2, random_state=35)\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "train.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Q4FTu_Os0nVu",
        "outputId": "526f94eb-a9c4-47f9-d1fe-b4c248782874"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>years clutch oscar nominees least popular  yea...</td>\n",
              "      <td>1</td>\n",
              "      <td>[years clutch oscar nominees least popular yea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>geek gadget fan next  months look like going l...</td>\n",
              "      <td>4</td>\n",
              "      <td>[geek gadget fan next months look like going l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         text_split\n",
              "0  years clutch oscar nominees least popular  yea...  ...  [years clutch oscar nominees least popular yea...\n",
              "1  geek gadget fan next  months look like going l...  ...  [geek gadget fan next months look like going l...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get labels\n",
        "label_list = [x for x in np.unique(train.label)]\n",
        "label_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5tHLTMO0qyP",
        "outputId": "959068f8-df16-4c4c-d074-d7bc43ba4ca3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val.reset_index(drop=True, inplace=True)\n",
        "val.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "gCoftGPk0rnI",
        "outputId": "6aa223dd-7316-46e3-c238-293c8ee141e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>actor keanu reeves best known role matrix movi...</td>\n",
              "      <td>1</td>\n",
              "      <td>[actor keanu reeves best known role matrix mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>researchers monash swinburne rmit universities...</td>\n",
              "      <td>4</td>\n",
              "      <td>[researchers monash swinburne rmit universitie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         text_split\n",
              "0  actor keanu reeves best known role matrix movi...  ...  [actor keanu reeves best known role matrix mov...\n",
              "1  researchers monash swinburne rmit universities...  ...  [researchers monash swinburne rmit universitie...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val.shape, train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2larxPI60vzX",
        "outputId": "01d1e125-e748-4fd1-fc80-52d38716963a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 3), (400, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_l = []\n",
        "label_l = []\n",
        "index_l =[]\n",
        "for idx,row in train.iterrows():\n",
        "    for l in row['text_split']:\n",
        "        train_l.append(l)\n",
        "        label_l.append(row['label'])\n",
        "        index_l.append(idx)\n",
        "len(train_l), len(label_l), len(index_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALG5MCK700vn",
        "outputId": "a4352019-e9c3-429c-f138-d907e7056b60"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(469, 469, 469)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_l = []\n",
        "val_label_l = []\n",
        "val_index_l = []\n",
        "for idx,row in val.iterrows():\n",
        "    for l in row['text_split']:\n",
        "        val_l.append(l)\n",
        "        val_label_l.append(row['label'])\n",
        "        val_index_l.append(idx)\n",
        "len(val_l), len(val_label_l), len(val_index_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMPZdY4G03v4",
        "outputId": "1270b539-1a04-4d51-8c1e-fa2f3279fff2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(126, 126, 126)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame({DATA_COLUMN:train_l, LABEL_COLUMN:label_l})\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1KmXRcZO05Jv",
        "outputId": "ac647b0a-f453-41bb-a86a-edc95433a79f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>years clutch oscar nominees least popular year...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>geek gadget fan next months look like going lo...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>transmission speeds megabits per second works ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lessons music piracy copyright issues taught s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>katerina thanou confident fellow sprinter kost...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  years clutch oscar nominees least popular year...      1\n",
              "1  geek gadget fan next months look like going lo...      4\n",
              "2  transmission speeds megabits per second works ...      4\n",
              "3  lessons music piracy copyright issues taught s...      1\n",
              "4  katerina thanou confident fellow sprinter kost...      3"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.DataFrame({DATA_COLUMN:val_l, LABEL_COLUMN:val_label_l})\n",
        "val_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eiLRsyFz05Mo",
        "outputId": "a53106ed-2dc0-49de-a00b-131b3d65f86b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>actor keanu reeves best known role matrix movi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>researchers monash swinburne rmit universities...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>us dollar continued recordbreaking slide tumbl...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>may know quite describe position british polit...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>friends actress lisa kudrow play lead role new...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  actor keanu reeves best known role matrix movi...      1\n",
              "1  researchers monash swinburne rmit universities...      4\n",
              "2  us dollar continued recordbreaking slide tumbl...      0\n",
              "3  may know quite describe position british polit...      2\n",
              "4  friends actress lisa kudrow play lead role new...      1"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "Z0-jpi8I05O5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "val_InputExamples = val.apply(lambda x: run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "metadata": {
        "id": "iqnO-VxZ05T-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_InputExamples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRMdp6xn1Iyo",
        "outputId": "fde02a29-6399-4144-a3b4-194e32ba6774"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      <run_classifier.InputExample object at 0x7f117...\n",
              "1      <run_classifier.InputExample object at 0x7f117...\n",
              "2      <run_classifier.InputExample object at 0x7f117...\n",
              "3      <run_classifier.InputExample object at 0x7f117...\n",
              "4      <run_classifier.InputExample object at 0x7f117...\n",
              "                             ...                        \n",
              "395    <run_classifier.InputExample object at 0x7f117...\n",
              "396    <run_classifier.InputExample object at 0x7f117...\n",
              "397    <run_classifier.InputExample object at 0x7f117...\n",
              "398    <run_classifier.InputExample object at 0x7f117...\n",
              "399    <run_classifier.InputExample object at 0x7f117...\n",
              "Length: 400, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\n",
        "print(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\n",
        "print(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\n",
        "print(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rn-LGON1MZX",
        "outputId": "3e78a38e-287c-4001-c86f-6ebe924dc29c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 0 - guid of training set :  None\n",
            "\n",
            "__________\n",
            "Row 0 - text_a of training set :  years clutch oscar nominees least popular  years according box office figures us five nominated best film seen  fewer people movies previous years awards based box office popularity concern ratings televised ceremony dont titanic lord rings think fair say concern us bit said academy executive director bruce davis  million people us seen years nominees compared  million  million recent years last time combined attendance low  amadeus beat killing fields passage india places heart soldiers story best picture  million saw five films last years ceremony attracted highest audience four years viewers tuned see lord ring return king sweep board show reaped biggest audience  titanic took home  oscars film taken m m worldwide ceremony eventually took bn m eyeballs starring movie screen translates eyeballs staring tv screen said paul dergarabedian box office tracker exhibitor relations people like vested interest theyre watching titanic bn worldwide box office youve got lot people vested interest past years also seen blockbusters saving private ryan forrest gump ghost compete oscars biggest box office hitter among years nominees aviator taken m m us although takings uk reached m far lowbudget move sideways finding neverland far grossed m m years biggest blockbusters actually feature oscar nominees animation category shrek  incredibles took m m incredibles took m m mel gibsons passion christ took m m us largely ignored academy voters many film industry equate award box office success never equated academy awards much money movie takes said nikki rocco head distribution universal released nominee ray thats peoples choice awards public industry bestowing awards think best films year\n",
            "\n",
            "__________\n",
            "Row 0 - text_b of training set :  None\n",
            "\n",
            "__________\n",
            "Row 0 - label of training set :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "#     with tf.compat.v1.Session() as sess:\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "    return tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCu07OjQ1P4A",
        "outputId": "27c9ef0e-ad49-4e1b-b521-b448a5759724"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab.keys())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhXVu0Bi1XQH",
        "outputId": "7c81ff1a-1dd0-485f-fa22-0a8983908b89"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here is what the tokenised sample of the first training set observation looks like\n",
        "print(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU-wCayu1bD3",
        "outputId": "ca5d2f95-ec89-466d-a7ed-443e1276813f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['years', 'clutch', 'oscar', 'nominees', 'least', 'popular', 'years', 'according', 'box', 'office', 'figures', 'us', 'five', 'nominated', 'best', 'film', 'seen', 'fewer', 'people', 'movies', 'previous', 'years', 'awards', 'based', 'box', 'office', 'popularity', 'concern', 'ratings', 'televised', 'ceremony', 'don', '##t', 'titanic', 'lord', 'rings', 'think', 'fair', 'say', 'concern', 'us', 'bit', 'said', 'academy', 'executive', 'director', 'bruce', 'davis', 'million', 'people', 'us', 'seen', 'years', 'nominees', 'compared', 'million', 'million', 'recent', 'years', 'last', 'time', 'combined', 'attendance', 'low', 'amadeus', 'beat', 'killing', 'fields', 'passage', 'india', 'places', 'heart', 'soldiers', 'story', 'best', 'picture', 'million', 'saw', 'five', 'films', 'last', 'years', 'ceremony', 'attracted', 'highest', 'audience', 'four', 'years', 'viewers', 'tuned', 'see', 'lord', 'ring', 'return', 'king', 'sweep', 'board', 'show', 're', '##ape', '##d', 'biggest', 'audience', 'titanic', 'took', 'home', 'oscar', '##s', 'film', 'taken', 'm', 'm', 'worldwide', 'ceremony', 'eventually', 'took', 'bn', 'm', 'eye', '##balls', 'starring', 'movie', 'screen', 'translates', 'eye', '##balls', 'staring', 'tv', 'screen', 'said', 'paul', 'der', '##gara', '##bed', '##ian', 'box', 'office', 'tracker', 'exhibit', '##or', 'relations', 'people', 'like', 'vested', 'interest', 'they', '##re', 'watching', 'titanic', 'bn', 'worldwide', 'box', 'office', 'you', '##ve', 'got', 'lot', 'people', 'vested', 'interest', 'past', 'years', 'also', 'seen', 'blockbuster', '##s', 'saving', 'private', 'ryan', 'forrest', 'gum', '##p', 'ghost', 'compete', 'oscar', '##s', 'biggest', 'box', 'office', 'hitter', 'among', 'years', 'nominees', 'aviator', 'taken', 'm', 'm', 'us', 'although', 'taking', '##s', 'uk', 'reached', 'm', 'far', 'low', '##bu', '##dget', 'move', 'sideways', 'finding', 'never', '##land', 'far', 'grossed', 'm', 'm', 'years', 'biggest', 'blockbuster', '##s', 'actually', 'feature', 'oscar', 'nominees', 'animation', 'category', 'sh', '##rek', 'incredible', '##s', 'took', 'm', 'm', 'incredible', '##s', 'took', 'm', 'm', 'mel', 'gibson', '##s', 'passion', 'christ', 'took', 'm', 'm', 'us', 'largely', 'ignored', 'academy', 'voters', 'many', 'film', 'industry', 'e', '##qua', '##te', 'award', 'box', 'office', 'success', 'never', 'e', '##qua', '##ted', 'academy', 'awards', 'much', 'money', 'movie', 'takes', 'said', 'nikki', 'rocco', 'head', 'distribution', 'universal', 'released', 'nominee', 'ray', 'that', '##s', 'peoples', 'choice', 'awards', 'public', 'industry', 'best', '##owing', 'awards', 'think', 'best', 'films', 'year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 200\n",
        "\n",
        "train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "val_features = run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAEWirj01f9Q",
        "outputId": "cbb21227-3a3a-442d-fa11-3152c865f2bf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Writing example 0 of 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Writing example 0 of 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] years clutch oscar nominees least popular years according box office figures us five nominated best film seen fewer people movies previous years awards based box office popularity concern ratings televised ceremony don ##t titanic lord rings think fair say concern us bit said academy executive director bruce davis million people us seen years nominees compared million million recent years last time combined attendance low amadeus beat killing fields passage india places heart soldiers story best picture million saw five films last years ceremony attracted highest audience four years viewers tuned see lord ring return king sweep board show re ##ape ##d biggest audience titanic took home oscar ##s film taken m m worldwide ceremony eventually took bn m eye ##balls starring movie screen translates eye ##balls staring tv screen said paul der ##gara ##bed ##ian box office tracker exhibit ##or relations people like vested interest they ##re watching titanic bn worldwide box office you ##ve got lot people vested interest past years also seen blockbuster ##s saving private ryan forrest gum ##p ghost compete oscar ##s biggest box office hitter among years nominees aviator taken m m us although taking ##s uk reached m far low ##bu ##dget [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] years clutch oscar nominees least popular years according box office figures us five nominated best film seen fewer people movies previous years awards based box office popularity concern ratings televised ceremony don ##t titanic lord rings think fair say concern us bit said academy executive director bruce davis million people us seen years nominees compared million million recent years last time combined attendance low amadeus beat killing fields passage india places heart soldiers story best picture million saw five films last years ceremony attracted highest audience four years viewers tuned see lord ring return king sweep board show re ##ape ##d biggest audience titanic took home oscar ##s film taken m m worldwide ceremony eventually took bn m eye ##balls starring movie screen translates eye ##balls staring tv screen said paul der ##gara ##bed ##ian box office tracker exhibit ##or relations people like vested interest they ##re watching titanic bn worldwide box office you ##ve got lot people vested interest past years also seen blockbuster ##s saving private ryan forrest gum ##p ghost compete oscar ##s biggest box office hitter among years nominees aviator taken m m us although taking ##s uk reached m far low ##bu ##dget [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 2086 15357 7436 17853 2560 2759 2086 2429 3482 2436 4481 2149 2274 4222 2190 2143 2464 8491 2111 5691 3025 2086 2982 2241 3482 2436 6217 5142 8599 13762 5103 2123 2102 20753 2935 7635 2228 4189 2360 5142 2149 2978 2056 2914 3237 2472 5503 4482 2454 2111 2149 2464 2086 17853 4102 2454 2454 3522 2086 2197 2051 4117 5270 2659 27185 3786 4288 4249 6019 2634 3182 2540 3548 2466 2190 3861 2454 2387 2274 3152 2197 2086 5103 6296 3284 4378 2176 2086 7193 15757 2156 2935 3614 2709 2332 11740 2604 2265 2128 24065 2094 5221 4378 20753 2165 2188 7436 2015 2143 2579 1049 1049 4969 5103 2776 2165 24869 1049 3239 18510 4626 3185 3898 16315 3239 18510 4582 2694 3898 2056 2703 4315 24864 8270 2937 3482 2436 27080 8327 2953 4262 2111 2066 26003 3037 2027 2890 3666 20753 24869 4969 3482 2436 2017 3726 2288 2843 2111 26003 3037 2627 2086 2036 2464 27858 2015 7494 2797 4575 16319 16031 2361 5745 5566 7436 2015 5221 3482 2436 18694 2426 2086 17853 24035 2579 1049 1049 2149 2348 2635 2015 2866 2584 1049 2521 2659 8569 24291 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 2086 15357 7436 17853 2560 2759 2086 2429 3482 2436 4481 2149 2274 4222 2190 2143 2464 8491 2111 5691 3025 2086 2982 2241 3482 2436 6217 5142 8599 13762 5103 2123 2102 20753 2935 7635 2228 4189 2360 5142 2149 2978 2056 2914 3237 2472 5503 4482 2454 2111 2149 2464 2086 17853 4102 2454 2454 3522 2086 2197 2051 4117 5270 2659 27185 3786 4288 4249 6019 2634 3182 2540 3548 2466 2190 3861 2454 2387 2274 3152 2197 2086 5103 6296 3284 4378 2176 2086 7193 15757 2156 2935 3614 2709 2332 11740 2604 2265 2128 24065 2094 5221 4378 20753 2165 2188 7436 2015 2143 2579 1049 1049 4969 5103 2776 2165 24869 1049 3239 18510 4626 3185 3898 16315 3239 18510 4582 2694 3898 2056 2703 4315 24864 8270 2937 3482 2436 27080 8327 2953 4262 2111 2066 26003 3037 2027 2890 3666 20753 24869 4969 3482 2436 2017 3726 2288 2843 2111 26003 3037 2627 2086 2036 2464 27858 2015 7494 2797 4575 16319 16031 2361 5745 5566 7436 2015 5221 3482 2436 18694 2426 2086 17853 24035 2579 1049 1049 2149 2348 2635 2015 2866 2584 1049 2521 2659 8569 24291 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] geek ga ##dget fan next months look like going lot fun relentless pace development hit ##ech world rampant competition many sectors particularly among mobile phone firms suggests going good year begin year third ##gen ##eration g mobile phones become in ##es ##cap ##able network launched vo ##da ##fo ##ne launched consumer service november orange followed early december t ##mobile o due launch main result launches likely sl ##ew good deals consumers operators try po ##ach new customers rivals convince existing users trade already extra capacity g networks lets offer good deals voice calls rates probably matched operators shift technology low cost voice calls means operators lose significant chunk revenue show operator believes voice business sustain ill write obituary said ni ##el ransom chief technology officer al ##cate ##l instead operators likely push things g phones video messaging multimedia capabilities already camera phones look set challenge digital cameras likely win fans multi ##me ##ga ##pi ##x ##el devices go sale g everything way face competition emerging technologies wi ##max wireless technology boost data transmission speeds mega ##bit ##s per second works distances miles kent likely site uk ##s first wi ##max network due go live could way rural areas [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] geek ga ##dget fan next months look like going lot fun relentless pace development hit ##ech world rampant competition many sectors particularly among mobile phone firms suggests going good year begin year third ##gen ##eration g mobile phones become in ##es ##cap ##able network launched vo ##da ##fo ##ne launched consumer service november orange followed early december t ##mobile o due launch main result launches likely sl ##ew good deals consumers operators try po ##ach new customers rivals convince existing users trade already extra capacity g networks lets offer good deals voice calls rates probably matched operators shift technology low cost voice calls means operators lose significant chunk revenue show operator believes voice business sustain ill write obituary said ni ##el ransom chief technology officer al ##cate ##l instead operators likely push things g phones video messaging multimedia capabilities already camera phones look set challenge digital cameras likely win fans multi ##me ##ga ##pi ##x ##el devices go sale g everything way face competition emerging technologies wi ##max wireless technology boost data transmission speeds mega ##bit ##s per second works distances miles kent likely site uk ##s first wi ##max network due go live could way rural areas [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 29294 11721 24291 5470 2279 2706 2298 2066 2183 2843 4569 21660 6393 2458 2718 15937 2088 25883 2971 2116 11105 3391 2426 4684 3042 9786 6083 2183 2204 2095 4088 2095 2353 6914 16754 1043 4684 11640 2468 1999 2229 17695 3085 2897 3390 29536 2850 14876 2638 3390 7325 2326 2281 4589 2628 2220 2285 1056 17751 1051 2349 4888 2364 2765 18989 3497 22889 7974 2204 9144 10390 9224 3046 13433 6776 2047 6304 9169 8054 4493 5198 3119 2525 4469 3977 1043 6125 11082 3749 2204 9144 2376 4455 6165 2763 10349 9224 5670 2974 2659 3465 2376 4455 2965 9224 4558 3278 20000 6599 2265 6872 7164 2376 2449 15770 5665 4339 20815 2056 9152 2884 16540 2708 2974 2961 2632 16280 2140 2612 9224 3497 5245 2477 1043 11640 2678 24732 14959 9859 2525 4950 11640 2298 2275 4119 3617 8629 3497 2663 4599 4800 4168 3654 8197 2595 2884 5733 2175 5096 1043 2673 2126 2227 2971 8361 6786 15536 17848 9949 2974 12992 2951 6726 10898 13164 16313 2015 2566 2117 2573 12103 2661 5982 3497 2609 2866 2015 2034 15536 17848 2897 2349 2175 2444 2071 2126 3541 2752 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 29294 11721 24291 5470 2279 2706 2298 2066 2183 2843 4569 21660 6393 2458 2718 15937 2088 25883 2971 2116 11105 3391 2426 4684 3042 9786 6083 2183 2204 2095 4088 2095 2353 6914 16754 1043 4684 11640 2468 1999 2229 17695 3085 2897 3390 29536 2850 14876 2638 3390 7325 2326 2281 4589 2628 2220 2285 1056 17751 1051 2349 4888 2364 2765 18989 3497 22889 7974 2204 9144 10390 9224 3046 13433 6776 2047 6304 9169 8054 4493 5198 3119 2525 4469 3977 1043 6125 11082 3749 2204 9144 2376 4455 6165 2763 10349 9224 5670 2974 2659 3465 2376 4455 2965 9224 4558 3278 20000 6599 2265 6872 7164 2376 2449 15770 5665 4339 20815 2056 9152 2884 16540 2708 2974 2961 2632 16280 2140 2612 9224 3497 5245 2477 1043 11640 2678 24732 14959 9859 2525 4950 11640 2298 2275 4119 3617 8629 3497 2663 4599 4800 4168 3654 8197 2595 2884 5733 2175 5096 1043 2673 2126 2227 2971 8361 6786 15536 17848 9949 2974 12992 2951 6726 10898 13164 16313 2015 2566 2117 2573 12103 2661 5982 3497 2609 2866 2015 2034 15536 17848 2897 2349 2175 2444 2071 2126 3541 2752 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 4 (id = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 4 (id = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] lessons music piracy copyright issues taught secondary school pupils uk lessons aimed year ##old ##s introduce copyright including issues download ##ing internet illegal copying cds role protecting creativity music piracy including illegally swap ##ping music online costs uk music industry millions every year blamed decline worldwide cd sales british music rights b ##m ##r formed represent interests songwriters composers worked education experts put together learning pack songwriter guy chambers worked stars including robbie williams thrown support behind scheme said well educating children music piracy would also protect young people planning career music industry un ##sc ##rup ##ulous individuals debate london launch scheme chambers said think important young people receive practical engaging learning schools lessons give insight creative industries work help possible future careers education pack already requested secondary schools aimed giving children understanding copyright relation music industry also teach children importance royalties raises awareness different careers music industry particularly digital age henri yo ##xa ##ll general manager british music rights told bbc news schools crying resource help educate pupils issues scheme extension b ##m ##rs respect value music campaign also backed singers ##ong ##writer ##s fear ##gal shark ##ey lucie silva ##s grammy award ##win ##ning composer david [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] lessons music piracy copyright issues taught secondary school pupils uk lessons aimed year ##old ##s introduce copyright including issues download ##ing internet illegal copying cds role protecting creativity music piracy including illegally swap ##ping music online costs uk music industry millions every year blamed decline worldwide cd sales british music rights b ##m ##r formed represent interests songwriters composers worked education experts put together learning pack songwriter guy chambers worked stars including robbie williams thrown support behind scheme said well educating children music piracy would also protect young people planning career music industry un ##sc ##rup ##ulous individuals debate london launch scheme chambers said think important young people receive practical engaging learning schools lessons give insight creative industries work help possible future careers education pack already requested secondary schools aimed giving children understanding copyright relation music industry also teach children importance royalties raises awareness different careers music industry particularly digital age henri yo ##xa ##ll general manager british music rights told bbc news schools crying resource help educate pupils issues scheme extension b ##m ##rs respect value music campaign also backed singers ##ong ##writer ##s fear ##gal shark ##ey lucie silva ##s grammy award ##win ##ning composer david [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 8220 2189 24386 9385 3314 4036 3905 2082 7391 2866 8220 6461 2095 11614 2015 8970 9385 2164 3314 8816 2075 4274 6206 24731 14340 2535 8650 14842 2189 24386 2164 17800 19948 4691 2189 3784 5366 2866 2189 3068 8817 2296 2095 11248 6689 4969 3729 4341 2329 2189 2916 1038 2213 2099 2719 5050 5426 20602 9929 2499 2495 8519 2404 2362 4083 5308 6009 3124 8477 2499 3340 2164 12289 3766 6908 2490 2369 5679 2056 2092 25088 2336 2189 24386 2052 2036 4047 2402 2111 4041 2476 2189 3068 4895 11020 21531 16203 3633 5981 2414 4888 5679 8477 2056 2228 2590 2402 2111 4374 6742 11973 4083 2816 8220 2507 12369 5541 6088 2147 2393 2825 2925 10922 2495 5308 2525 7303 3905 2816 6461 3228 2336 4824 9385 7189 2189 3068 2036 6570 2336 5197 25335 13275 7073 2367 10922 2189 3068 3391 3617 2287 8863 10930 18684 3363 2236 3208 2329 2189 2916 2409 4035 2739 2816 6933 7692 2393 16957 7391 3314 5679 5331 1038 2213 2869 4847 3643 2189 3049 2036 6153 8453 5063 15994 2015 3571 9692 11420 3240 28831 11183 2015 8922 2400 10105 5582 4543 2585 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 8220 2189 24386 9385 3314 4036 3905 2082 7391 2866 8220 6461 2095 11614 2015 8970 9385 2164 3314 8816 2075 4274 6206 24731 14340 2535 8650 14842 2189 24386 2164 17800 19948 4691 2189 3784 5366 2866 2189 3068 8817 2296 2095 11248 6689 4969 3729 4341 2329 2189 2916 1038 2213 2099 2719 5050 5426 20602 9929 2499 2495 8519 2404 2362 4083 5308 6009 3124 8477 2499 3340 2164 12289 3766 6908 2490 2369 5679 2056 2092 25088 2336 2189 24386 2052 2036 4047 2402 2111 4041 2476 2189 3068 4895 11020 21531 16203 3633 5981 2414 4888 5679 8477 2056 2228 2590 2402 2111 4374 6742 11973 4083 2816 8220 2507 12369 5541 6088 2147 2393 2825 2925 10922 2495 5308 2525 7303 3905 2816 6461 3228 2336 4824 9385 7189 2189 3068 2036 6570 2336 5197 25335 13275 7073 2367 10922 2189 3068 3391 3617 2287 8863 10930 18684 3363 2236 3208 2329 2189 2916 2409 4035 2739 2816 6933 7692 2393 16957 7391 3314 5679 5331 1038 2213 2869 4847 3643 2189 3049 2036 6153 8453 5063 15994 2015 3571 9692 11420 3240 28831 11183 2015 8922 2400 10105 5582 4543 2585 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] kate ##rina than ##ou confident fellow sprinter ko ##sta ##s kent ##eri ##s punished missing drugs tests athens olympics greek pair appeared hearing saturday determine whether provisional ban ##s athletics ruling body iaaf stand five months finally chance give explanations confident optimistic said than ##ou presented new evidence committee aware athletes lawyer gr ##igo ##ris io ##ani ##dis said believed independent disciplinary committee set greek athletics federation sega ##s would find innocent almost certain charges dropped said io ##ani ##dis believe presented case charges unreasonable than ##ou olympic women ##s m silver medallist sydney m champion kent ##eri ##s suspended iaaf missing three drugs tests third supposed take place eve athens games last august pair could found athletes village later taken hospital claiming involved motorcycle accident than ##ous coach christ ##os t ##zek ##os also suspended iaaf asked disciplinary committee kinds questions night august said t ##zek ##os leave gaps far concerned issue refusing tested optimistic t ##zek ##os than ##ou kent ##eri ##s denied charges expect decision within month del ##ibe ##rations start additional documents brought thursday said committee chairman ko ##sta ##s pan ##ago ##poulos estimate final ruling issued end february [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] kate ##rina than ##ou confident fellow sprinter ko ##sta ##s kent ##eri ##s punished missing drugs tests athens olympics greek pair appeared hearing saturday determine whether provisional ban ##s athletics ruling body iaaf stand five months finally chance give explanations confident optimistic said than ##ou presented new evidence committee aware athletes lawyer gr ##igo ##ris io ##ani ##dis said believed independent disciplinary committee set greek athletics federation sega ##s would find innocent almost certain charges dropped said io ##ani ##dis believe presented case charges unreasonable than ##ou olympic women ##s m silver medallist sydney m champion kent ##eri ##s suspended iaaf missing three drugs tests third supposed take place eve athens games last august pair could found athletes village later taken hospital claiming involved motorcycle accident than ##ous coach christ ##os t ##zek ##os also suspended iaaf asked disciplinary committee kinds questions night august said t ##zek ##os leave gaps far concerned issue refusing tested optimistic t ##zek ##os than ##ou kent ##eri ##s denied charges expect decision within month del ##ibe ##rations start additional documents brought thursday said committee chairman ko ##sta ##s pan ##ago ##poulos estimate final ruling issued end february [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 5736 11796 2084 7140 9657 3507 19938 12849 9153 2015 5982 11124 2015 14248 4394 5850 5852 7571 3783 3306 3940 2596 4994 5095 5646 3251 10864 7221 2015 6482 6996 2303 21259 3233 2274 2706 2633 3382 2507 17959 9657 21931 2056 2084 7140 3591 2047 3350 2837 5204 7576 5160 24665 14031 6935 22834 7088 10521 2056 3373 2981 17972 2837 2275 3306 6482 4657 16562 2015 2052 2424 7036 2471 3056 5571 3333 2056 22834 7088 10521 2903 3591 2553 5571 29205 2084 7140 4386 2308 2015 1049 3165 28595 3994 1049 3410 5982 11124 2015 6731 21259 4394 2093 5850 5852 2353 4011 2202 2173 6574 7571 2399 2197 2257 3940 2071 2179 7576 2352 2101 2579 2902 6815 2920 9055 4926 2084 3560 2873 4828 2891 1056 24506 2891 2036 6731 21259 2356 17972 2837 7957 3980 2305 2257 2056 1056 24506 2891 2681 16680 2521 4986 3277 11193 7718 21931 1056 24506 2891 2084 7140 5982 11124 2015 6380 5571 5987 3247 2306 3204 3972 20755 28893 2707 3176 5491 2716 9432 2056 2837 3472 12849 9153 2015 6090 23692 24662 10197 2345 6996 3843 2203 2337 102 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 5736 11796 2084 7140 9657 3507 19938 12849 9153 2015 5982 11124 2015 14248 4394 5850 5852 7571 3783 3306 3940 2596 4994 5095 5646 3251 10864 7221 2015 6482 6996 2303 21259 3233 2274 2706 2633 3382 2507 17959 9657 21931 2056 2084 7140 3591 2047 3350 2837 5204 7576 5160 24665 14031 6935 22834 7088 10521 2056 3373 2981 17972 2837 2275 3306 6482 4657 16562 2015 2052 2424 7036 2471 3056 5571 3333 2056 22834 7088 10521 2903 3591 2553 5571 29205 2084 7140 4386 2308 2015 1049 3165 28595 3994 1049 3410 5982 11124 2015 6731 21259 4394 2093 5850 5852 2353 4011 2202 2173 6574 7571 2399 2197 2257 3940 2071 2179 7576 2352 2101 2579 2902 6815 2920 9055 4926 2084 3560 2873 4828 2891 1056 24506 2891 2036 6731 21259 2356 17972 2837 7957 3980 2305 2257 2056 1056 24506 2891 2681 16680 2521 4986 3277 11193 7718 21931 1056 24506 2891 2084 7140 5982 11124 2015 6380 5571 5987 3247 2306 3204 3972 20755 28893 2707 3176 5491 2716 9432 2056 2837 3472 12849 9153 2015 6090 23692 24662 10197 2345 6996 3843 2203 2337 102 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 3 (id = 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] indian telecommunications firm turned lasers help overcome problems setting voice data networks country tata tel ##ese ##r ##vic ##es using lasers make link customers offices core network laser bridges work across distances km set much faster cable connections months lasers helped firm set networks locations particular geography getting permission dig ground lay pipes bit task said mr r sri ##dhar ##an vice president networks tata heavy traffic layout ground mean digging uniquely difficult said locations said permission dig roads lay cables impossible get said far easier secure permission putting networking hardware roofs led chennai ##base ##d tata turn equipment uses lasers make final mile leap tata ##s core network premises customers light ##point ##e laser bridges work distances km used route voice data businesses backbone network hardware works pairs beam data air form laser pulses laser bridges route data speeds gb ##ps times faster kb ##ps broadband connection tata running hardware modest speeds mb ##ps lasers also ideal india climate particularly suitable rain rate little low hardly ever fog ##gy said places rain heavy fog common laser links struggle maintain good connection speeds laser links also take far less time set get working said mr sri ##dhar ##an [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] indian telecommunications firm turned lasers help overcome problems setting voice data networks country tata tel ##ese ##r ##vic ##es using lasers make link customers offices core network laser bridges work across distances km set much faster cable connections months lasers helped firm set networks locations particular geography getting permission dig ground lay pipes bit task said mr r sri ##dhar ##an vice president networks tata heavy traffic layout ground mean digging uniquely difficult said locations said permission dig roads lay cables impossible get said far easier secure permission putting networking hardware roofs led chennai ##base ##d tata turn equipment uses lasers make final mile leap tata ##s core network premises customers light ##point ##e laser bridges work distances km used route voice data businesses backbone network hardware works pairs beam data air form laser pulses laser bridges route data speeds gb ##ps times faster kb ##ps broadband connection tata running hardware modest speeds mb ##ps lasers also ideal india climate particularly suitable rain rate little low hardly ever fog ##gy said places rain heavy fog common laser links struggle maintain good connection speeds laser links also take far less time set get working said mr sri ##dhar ##an [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 2796 12108 3813 2357 23965 2393 9462 3471 4292 2376 2951 6125 2406 23236 10093 6810 2099 7903 2229 2478 23965 2191 4957 6304 4822 4563 2897 9138 7346 2147 2408 12103 2463 2275 2172 5514 5830 7264 2706 23965 3271 3813 2275 6125 5269 3327 10505 2893 6656 10667 2598 3913 12432 2978 4708 2056 2720 1054 5185 25632 2319 3580 2343 6125 23236 3082 4026 9621 2598 2812 10443 20640 3697 2056 5269 2056 6656 10667 4925 3913 15196 5263 2131 2056 2521 6082 5851 6656 5128 14048 8051 15753 2419 12249 15058 2094 23236 2735 3941 3594 23965 2191 2345 3542 11679 23236 2015 4563 2897 10345 6304 2422 8400 2063 9138 7346 2147 12103 2463 2109 2799 2376 2951 5661 21505 2897 8051 2573 7689 7504 2951 2250 2433 9138 23894 9138 7346 2799 2951 10898 16351 4523 2335 5514 21677 4523 19595 4434 23236 2770 8051 10754 10898 16914 4523 23965 2036 7812 2634 4785 3391 7218 4542 3446 2210 2659 6684 2412 9666 6292 2056 3182 4542 3082 9666 2691 9138 6971 5998 5441 2204 4434 10898 9138 6971 2036 2202 2521 2625 2051 2275 2131 2551 2056 2720 5185 25632 2319 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 2796 12108 3813 2357 23965 2393 9462 3471 4292 2376 2951 6125 2406 23236 10093 6810 2099 7903 2229 2478 23965 2191 4957 6304 4822 4563 2897 9138 7346 2147 2408 12103 2463 2275 2172 5514 5830 7264 2706 23965 3271 3813 2275 6125 5269 3327 10505 2893 6656 10667 2598 3913 12432 2978 4708 2056 2720 1054 5185 25632 2319 3580 2343 6125 23236 3082 4026 9621 2598 2812 10443 20640 3697 2056 5269 2056 6656 10667 4925 3913 15196 5263 2131 2056 2521 6082 5851 6656 5128 14048 8051 15753 2419 12249 15058 2094 23236 2735 3941 3594 23965 2191 2345 3542 11679 23236 2015 4563 2897 10345 6304 2422 8400 2063 9138 7346 2147 12103 2463 2109 2799 2376 2951 5661 21505 2897 8051 2573 7689 7504 2951 2250 2433 9138 23894 9138 7346 2799 2951 10898 16351 4523 2335 5514 21677 4523 19595 4434 23236 2770 8051 10754 10898 16914 4523 23965 2036 7812 2634 4785 3391 7218 4542 3446 2210 2659 6684 2412 9666 6292 2056 3182 4542 3082 9666 2691 9138 6971 5998 5441 2204 4434 10898 9138 6971 2036 2202 2521 2625 2051 2275 2131 2551 2056 2720 5185 25632 2319 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 4 (id = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 4 (id = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Writing example 0 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Writing example 0 of 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] actor ke ##anu reeves best known role matrix movies awarded star prestigious hollywood walk fame year ##old attended un ##ve ##iling star mother patricia thanked inspiring become actor years old asked mom ok actor reeves said said whatever want star th embedded pavement hollywood boulevard actor born lebanese capital beirut also spoke dropped school pursue acting career hollywood calling said got car british racing green volvo holes floor bricks holding seats young man full hopes dreams reeves first found fame teen comedy bill ted ##s excellent adventure went combine blockbuster ##s speed devils advocate matrix series smaller films including private idaho recently seen something ##s gotta give alongside jack nicholson diane ke ##aton next film supernatural thriller constantine released us later month opens uk march [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] actor ke ##anu reeves best known role matrix movies awarded star prestigious hollywood walk fame year ##old attended un ##ve ##iling star mother patricia thanked inspiring become actor years old asked mom ok actor reeves said said whatever want star th embedded pavement hollywood boulevard actor born lebanese capital beirut also spoke dropped school pursue acting career hollywood calling said got car british racing green volvo holes floor bricks holding seats young man full hopes dreams reeves first found fame teen comedy bill ted ##s excellent adventure went combine blockbuster ##s speed devils advocate matrix series smaller films including private idaho recently seen something ##s gotta give alongside jack nicholson diane ke ##aton next film supernatural thriller constantine released us later month opens uk march [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 3364 17710 24076 17891 2190 2124 2535 8185 5691 3018 2732 8919 5365 3328 4476 2095 11614 3230 4895 3726 16281 2732 2388 10717 15583 18988 2468 3364 2086 2214 2356 3566 7929 3364 17891 2056 2056 3649 2215 2732 16215 11157 14271 5365 8459 3364 2141 12592 3007 15335 2036 3764 3333 2082 7323 3772 2476 5365 4214 2056 2288 2482 2329 3868 2665 21074 8198 2723 14219 3173 4272 2402 2158 2440 8069 5544 17891 2034 2179 4476 9458 4038 3021 6945 2015 6581 6172 2253 11506 27858 2015 3177 13664 8175 8185 2186 3760 3152 2164 2797 9795 3728 2464 2242 2015 10657 2507 4077 2990 16955 12082 17710 22436 2279 2143 11189 10874 12790 2207 2149 2101 3204 7480 2866 2233 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 3364 17710 24076 17891 2190 2124 2535 8185 5691 3018 2732 8919 5365 3328 4476 2095 11614 3230 4895 3726 16281 2732 2388 10717 15583 18988 2468 3364 2086 2214 2356 3566 7929 3364 17891 2056 2056 3649 2215 2732 16215 11157 14271 5365 8459 3364 2141 12592 3007 15335 2036 3764 3333 2082 7323 3772 2476 5365 4214 2056 2288 2482 2329 3868 2665 21074 8198 2723 14219 3173 4272 2402 2158 2440 8069 5544 17891 2034 2179 4476 9458 4038 3021 6945 2015 6581 6172 2253 11506 27858 2015 3177 13664 8175 8185 2186 3760 3152 2164 2797 9795 3728 2464 2242 2015 10657 2507 4077 2990 16955 12082 17710 22436 2279 2143 11189 10874 12790 2207 2149 2101 3204 7480 2866 2233 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] researchers mona ##sh sw ##in ##burn ##e rm ##it universities successfully tested recorded australia ##s fastest internet data speed world single optical chip capable download ##ing high definition movies split second published prestigious journal nature communications findings potential fast ##tra ##ck next years australia ##s telecommunications capacity also possibility home ##gr ##own technology rolled across world technology capacity support highs ##peed internet connections million households melbourne australia time billions across world peak periods used new device replaces lasers one single piece equipment known micro ##comb smaller lighter existing telecommunications hardware planted load ##test ##ed using existing infrastructure mirrors used n ##bn first time micro ##comb used field trial possesses highest amount data produced single optical chip currently getting sneak ##pe ##ak infrastructure internet hold two three years time due unprecedented number people using internet remote work social ##izing streaming really showing us need able scale capacity internet connections said dr bill co ##rco ##ran cole ##ad author study lecturer electrical computer systems engineering mona ##sh university research demonstrates ability fibers already ground thanks n ##bn project backbone communications networks future we ##ve developed something scala ##ble meet future needs [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] researchers mona ##sh sw ##in ##burn ##e rm ##it universities successfully tested recorded australia ##s fastest internet data speed world single optical chip capable download ##ing high definition movies split second published prestigious journal nature communications findings potential fast ##tra ##ck next years australia ##s telecommunications capacity also possibility home ##gr ##own technology rolled across world technology capacity support highs ##peed internet connections million households melbourne australia time billions across world peak periods used new device replaces lasers one single piece equipment known micro ##comb smaller lighter existing telecommunications hardware planted load ##test ##ed using existing infrastructure mirrors used n ##bn first time micro ##comb used field trial possesses highest amount data produced single optical chip currently getting sneak ##pe ##ak infrastructure internet hold two three years time due unprecedented number people using internet remote work social ##izing streaming really showing us need able scale capacity internet connections said dr bill co ##rco ##ran cole ##ad author study lecturer electrical computer systems engineering mona ##sh university research demonstrates ability fibers already ground thanks n ##bn project backbone communications networks future we ##ve developed something scala ##ble meet future needs [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 6950 13813 4095 25430 2378 8022 2063 28549 4183 5534 5147 7718 2680 2660 2015 7915 4274 2951 3177 2088 2309 9380 9090 5214 8816 2075 2152 6210 5691 3975 2117 2405 8919 3485 3267 4806 9556 4022 3435 6494 3600 2279 2086 2660 2015 12108 3977 2036 6061 2188 16523 12384 2974 4565 2408 2088 2974 3977 2490 26836 25599 4274 7264 2454 3911 4940 2660 2051 25501 2408 2088 4672 6993 2109 2047 5080 20736 23965 2028 2309 3538 3941 2124 12702 18274 3760 9442 4493 12108 8051 8461 7170 22199 2098 2478 4493 6502 13536 2109 1050 24700 2034 2051 12702 18274 2109 2492 3979 14882 3284 3815 2951 2550 2309 9380 9090 2747 2893 13583 5051 4817 6502 4274 2907 2048 2093 2086 2051 2349 15741 2193 2111 2478 4274 6556 2147 2591 6026 11058 2428 4760 2149 2342 2583 4094 3977 4274 7264 2056 2852 3021 2522 29566 5521 5624 4215 3166 2817 9162 5992 3274 3001 3330 13813 4095 2118 2470 16691 3754 16662 2525 2598 4283 1050 24700 2622 21505 4806 6125 2925 2057 3726 2764 2242 26743 3468 3113 2925 3791 102 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 6950 13813 4095 25430 2378 8022 2063 28549 4183 5534 5147 7718 2680 2660 2015 7915 4274 2951 3177 2088 2309 9380 9090 5214 8816 2075 2152 6210 5691 3975 2117 2405 8919 3485 3267 4806 9556 4022 3435 6494 3600 2279 2086 2660 2015 12108 3977 2036 6061 2188 16523 12384 2974 4565 2408 2088 2974 3977 2490 26836 25599 4274 7264 2454 3911 4940 2660 2051 25501 2408 2088 4672 6993 2109 2047 5080 20736 23965 2028 2309 3538 3941 2124 12702 18274 3760 9442 4493 12108 8051 8461 7170 22199 2098 2478 4493 6502 13536 2109 1050 24700 2034 2051 12702 18274 2109 2492 3979 14882 3284 3815 2951 2550 2309 9380 9090 2747 2893 13583 5051 4817 6502 4274 2907 2048 2093 2086 2051 2349 15741 2193 2111 2478 4274 6556 2147 2591 6026 11058 2428 4760 2149 2342 2583 4094 3977 4274 7264 2056 2852 3021 2522 29566 5521 5624 4215 3166 2817 9162 5992 3274 3001 3330 13813 4095 2118 2470 16691 3754 16662 2525 2598 4283 1050 24700 2622 21505 4806 6125 2925 2057 3726 2764 2242 26743 3468 3113 2925 3791 102 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 4 (id = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 4 (id = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] us dollar continued record ##break ##ing slide tumbled new low euro investors betting european central bank ec ##b anything weaken euro us thought favour declining dollar us struggling balloon ##ing trade deficit analysts said one easiest ways fund allowing de ##pre ##ciation dollar predicted dollar likely fall even us currency trading per euro gm ##t monday compares euro late trading new york friday record low dollar weakened sharply since september traded euro lost year japanese yen traders said thin trading levels amplified mondays move going take much push dollar one way said grant wilson mellon bank liquid ##ity measure number parties willing trade market half normal working day traders said [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] us dollar continued record ##break ##ing slide tumbled new low euro investors betting european central bank ec ##b anything weaken euro us thought favour declining dollar us struggling balloon ##ing trade deficit analysts said one easiest ways fund allowing de ##pre ##ciation dollar predicted dollar likely fall even us currency trading per euro gm ##t monday compares euro late trading new york friday record low dollar weakened sharply since september traded euro lost year japanese yen traders said thin trading levels amplified mondays move going take much push dollar one way said grant wilson mellon bank liquid ##ity measure number parties willing trade market half normal working day traders said [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 2149 7922 2506 2501 23890 2075 7358 18303 2047 2659 9944 9387 19244 2647 2430 2924 14925 2497 2505 23021 9944 2149 2245 7927 13993 7922 2149 8084 13212 2075 3119 15074 18288 2056 2028 25551 3971 4636 4352 2139 28139 23247 7922 10173 7922 3497 2991 2130 2149 9598 6202 2566 9944 13938 2102 6928 22963 9944 2397 6202 2047 2259 5958 2501 2659 7922 11855 9249 2144 2244 7007 9944 2439 2095 2887 18371 13066 2056 4857 6202 3798 26986 28401 2693 2183 2202 2172 5245 7922 2028 2126 2056 3946 4267 22181 2924 6381 3012 5468 2193 4243 5627 3119 3006 2431 3671 2551 2154 13066 2056 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 2149 7922 2506 2501 23890 2075 7358 18303 2047 2659 9944 9387 19244 2647 2430 2924 14925 2497 2505 23021 9944 2149 2245 7927 13993 7922 2149 8084 13212 2075 3119 15074 18288 2056 2028 25551 3971 4636 4352 2139 28139 23247 7922 10173 7922 3497 2991 2130 2149 9598 6202 2566 9944 13938 2102 6928 22963 9944 2397 6202 2047 2259 5958 2501 2659 7922 11855 9249 2144 2244 7007 9944 2439 2095 2887 18371 13066 2056 4857 6202 3798 26986 28401 2693 2183 2202 2172 5245 7922 2028 2126 2056 3946 4267 22181 2924 6381 3012 5468 2193 4243 5627 3119 3006 2431 3671 2551 2154 13066 2056 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] may know quite describe position british politics real opposition valid opposition effective opposition authentic opposition liberal democrats entering expected election campaign determined prove tori ##es real threat labour tory leader michael howard kicked election campaign li ##b dem leader charles kennedy dismissed conservatives fading force insisted limit party ##s ambitions coming poll produce pretty impressive recent election results back optimism election believes might see party big things sensible third party leaders usually cautious making predictions come big test mr kennedy falling trap fired campaign party ##s westminster hq would make predictions expected li ##b dem ##s increase votes commons seats election gap labour tori ##es narrow proved claimed defect ##ion labour robert jackson party offered genuine alternative war iraq identity cards student fees council tax nothing chose two big parties whereas liberal democrats offered cost ##ed sensible alternatives also happy break two party consensus taxation promising increase income tax earning year pay scrap ##ping student fees introduction free personal care elderly replacing council tax local income tax also happy offer pledge would deals prop either big parties election time optimistic sounds coming third party could dismissed self ##del ##usion longer case liberal democrats parties still insist li ##b [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] may know quite describe position british politics real opposition valid opposition effective opposition authentic opposition liberal democrats entering expected election campaign determined prove tori ##es real threat labour tory leader michael howard kicked election campaign li ##b dem leader charles kennedy dismissed conservatives fading force insisted limit party ##s ambitions coming poll produce pretty impressive recent election results back optimism election believes might see party big things sensible third party leaders usually cautious making predictions come big test mr kennedy falling trap fired campaign party ##s westminster hq would make predictions expected li ##b dem ##s increase votes commons seats election gap labour tori ##es narrow proved claimed defect ##ion labour robert jackson party offered genuine alternative war iraq identity cards student fees council tax nothing chose two big parties whereas liberal democrats offered cost ##ed sensible alternatives also happy break two party consensus taxation promising increase income tax earning year pay scrap ##ping student fees introduction free personal care elderly replacing council tax local income tax also happy offer pledge would deals prop either big parties election time optimistic sounds coming third party could dismissed self ##del ##usion longer case liberal democrats parties still insist li ##b [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 2089 2113 3243 6235 2597 2329 4331 2613 4559 9398 4559 4621 4559 14469 4559 4314 8037 5738 3517 2602 3049 4340 6011 23413 2229 2613 5081 4428 17117 3003 2745 4922 6476 2602 3049 5622 2497 17183 3003 2798 5817 7219 11992 14059 2486 7278 5787 2283 2015 19509 2746 8554 3965 3492 8052 3522 2602 3463 2067 27451 2602 7164 2453 2156 2283 2502 2477 21082 2353 2283 4177 2788 17145 2437 20932 2272 2502 3231 2720 5817 4634 8132 5045 3049 2283 2015 9434 16260 2052 2191 20932 3517 5622 2497 17183 2015 3623 4494 7674 4272 2602 6578 4428 23413 2229 4867 4928 3555 21262 3258 4428 2728 4027 2283 3253 10218 4522 2162 5712 4767 5329 3076 9883 2473 4171 2498 4900 2048 2502 4243 6168 4314 8037 3253 3465 2098 21082 15955 2036 3407 3338 2048 2283 10465 14952 10015 3623 3318 4171 7414 2095 3477 15121 4691 3076 9883 4955 2489 3167 2729 9750 6419 2473 4171 2334 3318 4171 2036 3407 3749 16393 2052 9144 17678 2593 2502 4243 2602 2051 21931 4165 2746 2353 2283 2071 7219 2969 9247 14499 2936 2553 4314 8037 4243 2145 18292 5622 2497 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 2089 2113 3243 6235 2597 2329 4331 2613 4559 9398 4559 4621 4559 14469 4559 4314 8037 5738 3517 2602 3049 4340 6011 23413 2229 2613 5081 4428 17117 3003 2745 4922 6476 2602 3049 5622 2497 17183 3003 2798 5817 7219 11992 14059 2486 7278 5787 2283 2015 19509 2746 8554 3965 3492 8052 3522 2602 3463 2067 27451 2602 7164 2453 2156 2283 2502 2477 21082 2353 2283 4177 2788 17145 2437 20932 2272 2502 3231 2720 5817 4634 8132 5045 3049 2283 2015 9434 16260 2052 2191 20932 3517 5622 2497 17183 2015 3623 4494 7674 4272 2602 6578 4428 23413 2229 4867 4928 3555 21262 3258 4428 2728 4027 2283 3253 10218 4522 2162 5712 4767 5329 3076 9883 2473 4171 2498 4900 2048 2502 4243 6168 4314 8037 3253 3465 2098 21082 15955 2036 3407 3338 2048 2283 10465 14952 10015 3623 3318 4171 7414 2095 3477 15121 4691 3076 9883 4955 2489 3167 2729 9750 6419 2473 4171 2334 3318 4171 2036 3407 3749 16393 2052 9144 17678 2593 2502 4243 2602 2051 21931 4165 2746 2353 2283 2071 7219 2969 9247 14499 2936 2553 4314 8037 4243 2145 18292 5622 2497 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 2 (id = 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:tokens: [CLS] friends actress lisa ku ##dro ##w play lead role new series one ##time sitcom star according hollywood reporter thirteen episodes comeback commissioned cable channel hbo home hits sex city ku ##dro ##w played phoebe friends cow ##rot ##e pilot episode also act executive producer hbo looking next big comedy hit since sex city drew close us february comeback first minute comedy series channel picked since sex city drew end six ##year ##run friends ended year run nbc network may attention ##s turned projects six individual stars would pursue matt le ##bla ##nc starring friends spin ##off sitcom charting joey ##s fortunes los angeles pursue ##s acting career jennifer an ##isto ##n rachel long ##run ##ning show enjoyed series successful film appearances projects pipeline court ##ene ##y cox ar ##quette monica working drama project along husband david ar ##quette hbo called rise fall taylor kennedy matthew perry played chandler appeared west end stage film beginning wisdom currently production david sc ##h ##wi ##mmer ross directed time friends also worked joey [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:tokens: [CLS] friends actress lisa ku ##dro ##w play lead role new series one ##time sitcom star according hollywood reporter thirteen episodes comeback commissioned cable channel hbo home hits sex city ku ##dro ##w played phoebe friends cow ##rot ##e pilot episode also act executive producer hbo looking next big comedy hit since sex city drew close us february comeback first minute comedy series channel picked since sex city drew end six ##year ##run friends ended year run nbc network may attention ##s turned projects six individual stars would pursue matt le ##bla ##nc starring friends spin ##off sitcom charting joey ##s fortunes los angeles pursue ##s acting career jennifer an ##isto ##n rachel long ##run ##ning show enjoyed series successful film appearances projects pipeline court ##ene ##y cox ar ##quette monica working drama project along husband david ar ##quette hbo called rise fall taylor kennedy matthew perry played chandler appeared west end stage film beginning wisdom currently production david sc ##h ##wi ##mmer ross directed time friends also worked joey [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_ids: 101 2814 3883 7059 13970 22196 2860 2377 2599 2535 2047 2186 2028 7292 13130 2732 2429 5365 6398 7093 4178 12845 4837 5830 3149 14633 2188 4978 3348 2103 13970 22196 2860 2209 18188 2814 11190 21709 2063 4405 2792 2036 2552 3237 3135 14633 2559 2279 2502 4038 2718 2144 3348 2103 3881 2485 2149 2337 12845 2034 3371 4038 2186 3149 3856 2144 3348 2103 3881 2203 2416 29100 15532 2814 3092 2095 2448 6788 2897 2089 3086 2015 2357 3934 2416 3265 3340 2052 7323 4717 3393 28522 12273 4626 2814 6714 7245 13130 17918 9558 2015 18023 3050 3349 7323 2015 3772 2476 7673 2019 20483 2078 5586 2146 15532 5582 2265 5632 2186 3144 2143 3922 3934 13117 2457 8625 2100 9574 12098 29416 9018 2551 3689 2622 2247 3129 2585 12098 29416 14633 2170 4125 2991 4202 5817 5487 6890 2209 13814 2596 2225 2203 2754 2143 2927 9866 2747 2537 2585 8040 2232 9148 15810 5811 2856 2051 2814 2036 2499 9558 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_ids: 101 2814 3883 7059 13970 22196 2860 2377 2599 2535 2047 2186 2028 7292 13130 2732 2429 5365 6398 7093 4178 12845 4837 5830 3149 14633 2188 4978 3348 2103 13970 22196 2860 2209 18188 2814 11190 21709 2063 4405 2792 2036 2552 3237 3135 14633 2559 2279 2502 4038 2718 2144 3348 2103 3881 2485 2149 2337 12845 2034 3371 4038 2186 3149 3856 2144 3348 2103 3881 2203 2416 29100 15532 2814 3092 2095 2448 6788 2897 2089 3086 2015 2357 3934 2416 3265 3340 2052 7323 4717 3393 28522 12273 4626 2814 6714 7245 13130 17918 9558 2015 18023 3050 3349 7323 2015 3772 2476 7673 2019 20483 2078 5586 2146 15532 5582 2265 5632 2186 3144 2143 3922 3934 13117 2457 8625 2100 9574 12098 29416 9018 2551 3689 2622 2247 3129 2585 12098 29416 14633 2170 4125 2991 4202 5817 5487 6890 2209 13814 2596 2225 2203 2754 2143 2927 9866 2747 2537 2585 8040 2232 9148 15810 5811 2856 2051 2814 2036 2499 9558 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example on first observation in the training set\n",
        "print(\"Sentence : \", train_InputExamples.iloc[0].text_a)\n",
        "print(\"-\"*30)\n",
        "print(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\n",
        "print(\"-\"*30)\n",
        "print(\"Input IDs : \", train_features[0].input_ids)\n",
        "print(\"-\"*30)\n",
        "print(\"Input Masks : \", train_features[0].input_mask)\n",
        "print(\"-\"*30)\n",
        "print(\"Segment IDs : \", train_features[0].segment_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLNNL_4B1jrG",
        "outputId": "615084b9-cc91-43e5-e4f4-7fd3509bd091"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence :  years clutch oscar nominees least popular  years according box office figures us five nominated best film seen  fewer people movies previous years awards based box office popularity concern ratings televised ceremony dont titanic lord rings think fair say concern us bit said academy executive director bruce davis  million people us seen years nominees compared  million  million recent years last time combined attendance low  amadeus beat killing fields passage india places heart soldiers story best picture  million saw five films last years ceremony attracted highest audience four years viewers tuned see lord ring return king sweep board show reaped biggest audience  titanic took home  oscars film taken m m worldwide ceremony eventually took bn m eyeballs starring movie screen translates eyeballs staring tv screen said paul dergarabedian box office tracker exhibitor relations people like vested interest theyre watching titanic bn worldwide box office youve got lot people vested interest past years also seen blockbusters saving private ryan forrest gump ghost compete oscars biggest box office hitter among years nominees aviator taken m m us although takings uk reached m far lowbudget move sideways finding neverland far grossed m m years biggest blockbusters actually feature oscar nominees animation category shrek  incredibles took m m incredibles took m m mel gibsons passion christ took m m us largely ignored academy voters many film industry equate award box office success never equated academy awards much money movie takes said nikki rocco head distribution universal released nominee ray thats peoples choice awards public industry bestowing awards think best films year\n",
            "------------------------------\n",
            "Tokens :  ['years', 'clutch', 'oscar', 'nominees', 'least', 'popular', 'years', 'according', 'box', 'office', 'figures', 'us', 'five', 'nominated', 'best', 'film', 'seen', 'fewer', 'people', 'movies', 'previous', 'years', 'awards', 'based', 'box', 'office', 'popularity', 'concern', 'ratings', 'televised', 'ceremony', 'don', '##t', 'titanic', 'lord', 'rings', 'think', 'fair', 'say', 'concern', 'us', 'bit', 'said', 'academy', 'executive', 'director', 'bruce', 'davis', 'million', 'people', 'us', 'seen', 'years', 'nominees', 'compared', 'million', 'million', 'recent', 'years', 'last', 'time', 'combined', 'attendance', 'low', 'amadeus', 'beat', 'killing', 'fields', 'passage', 'india', 'places', 'heart', 'soldiers', 'story', 'best', 'picture', 'million', 'saw', 'five', 'films', 'last', 'years', 'ceremony', 'attracted', 'highest', 'audience', 'four', 'years', 'viewers', 'tuned', 'see', 'lord', 'ring', 'return', 'king', 'sweep', 'board', 'show', 're', '##ape', '##d', 'biggest', 'audience', 'titanic', 'took', 'home', 'oscar', '##s', 'film', 'taken', 'm', 'm', 'worldwide', 'ceremony', 'eventually', 'took', 'bn', 'm', 'eye', '##balls', 'starring', 'movie', 'screen', 'translates', 'eye', '##balls', 'staring', 'tv', 'screen', 'said', 'paul', 'der', '##gara', '##bed', '##ian', 'box', 'office', 'tracker', 'exhibit', '##or', 'relations', 'people', 'like', 'vested', 'interest', 'they', '##re', 'watching', 'titanic', 'bn', 'worldwide', 'box', 'office', 'you', '##ve', 'got', 'lot', 'people', 'vested', 'interest', 'past', 'years', 'also', 'seen', 'blockbuster', '##s', 'saving', 'private', 'ryan', 'forrest', 'gum', '##p', 'ghost', 'compete', 'oscar', '##s', 'biggest', 'box', 'office', 'hitter', 'among', 'years', 'nominees', 'aviator', 'taken', 'm', 'm', 'us', 'although', 'taking', '##s', 'uk', 'reached', 'm', 'far', 'low', '##bu', '##dget', 'move', 'sideways', 'finding', 'never', '##land', 'far', 'grossed', 'm', 'm', 'years', 'biggest', 'blockbuster', '##s', 'actually', 'feature', 'oscar', 'nominees', 'animation', 'category', 'sh', '##rek', 'incredible', '##s', 'took', 'm', 'm', 'incredible', '##s', 'took', 'm', 'm', 'mel', 'gibson', '##s', 'passion', 'christ', 'took', 'm', 'm', 'us', 'largely', 'ignored', 'academy', 'voters', 'many', 'film', 'industry', 'e', '##qua', '##te', 'award', 'box', 'office', 'success', 'never', 'e', '##qua', '##ted', 'academy', 'awards', 'much', 'money', 'movie', 'takes', 'said', 'nikki', 'rocco', 'head', 'distribution', 'universal', 'released', 'nominee', 'ray', 'that', '##s', 'peoples', 'choice', 'awards', 'public', 'industry', 'best', '##owing', 'awards', 'think', 'best', 'films', 'year']\n",
            "------------------------------\n",
            "Input IDs :  [101, 2086, 15357, 7436, 17853, 2560, 2759, 2086, 2429, 3482, 2436, 4481, 2149, 2274, 4222, 2190, 2143, 2464, 8491, 2111, 5691, 3025, 2086, 2982, 2241, 3482, 2436, 6217, 5142, 8599, 13762, 5103, 2123, 2102, 20753, 2935, 7635, 2228, 4189, 2360, 5142, 2149, 2978, 2056, 2914, 3237, 2472, 5503, 4482, 2454, 2111, 2149, 2464, 2086, 17853, 4102, 2454, 2454, 3522, 2086, 2197, 2051, 4117, 5270, 2659, 27185, 3786, 4288, 4249, 6019, 2634, 3182, 2540, 3548, 2466, 2190, 3861, 2454, 2387, 2274, 3152, 2197, 2086, 5103, 6296, 3284, 4378, 2176, 2086, 7193, 15757, 2156, 2935, 3614, 2709, 2332, 11740, 2604, 2265, 2128, 24065, 2094, 5221, 4378, 20753, 2165, 2188, 7436, 2015, 2143, 2579, 1049, 1049, 4969, 5103, 2776, 2165, 24869, 1049, 3239, 18510, 4626, 3185, 3898, 16315, 3239, 18510, 4582, 2694, 3898, 2056, 2703, 4315, 24864, 8270, 2937, 3482, 2436, 27080, 8327, 2953, 4262, 2111, 2066, 26003, 3037, 2027, 2890, 3666, 20753, 24869, 4969, 3482, 2436, 2017, 3726, 2288, 2843, 2111, 26003, 3037, 2627, 2086, 2036, 2464, 27858, 2015, 7494, 2797, 4575, 16319, 16031, 2361, 5745, 5566, 7436, 2015, 5221, 3482, 2436, 18694, 2426, 2086, 17853, 24035, 2579, 1049, 1049, 2149, 2348, 2635, 2015, 2866, 2584, 1049, 2521, 2659, 8569, 24291, 102]\n",
            "------------------------------\n",
            "Input Masks :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "------------------------------\n",
            "Segment IDs :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \n",
        "    bert_module = hub.Module(\n",
        "        BERT_MODEL_HUB,\n",
        "        trainable=True)\n",
        "    bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "    bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "    # Use \"sequence_outputs\" for token-level output.\n",
        "    output_layer = bert_outputs[\"pooled_output\"]\n",
        "    # with tf.Session() as sess:\n",
        "    output_layer1 = bert_outputs[\"pooled_output\"]\n",
        "    # output_layer1 = 999\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "    # Create our own layer to tune for politeness data.\n",
        "    output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "\n",
        "        # Dropout helps prevent overfitting\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n",
        "\n",
        "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "        logits = tf.nn.bias_add(logits, output_bias)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "        # Convert labels into one-hot encoding\n",
        "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "        if is_predicting:\n",
        "            return (predicted_labels, log_probs, output_layer1)\n",
        "\n",
        "        # If we're train/eval, compute loss between predicted and actual label\n",
        "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "        loss = tf.reduce_mean(per_example_loss)\n",
        "        \n",
        "        return (loss, predicted_labels, log_probs)"
      ],
      "metadata": {
        "id": "JhiT7UVm1r23"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "    \n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "        # TRAIN and EVAL\n",
        "        if not is_predicting:\n",
        "\n",
        "            (loss, predicted_labels, log_probs) = create_model(\n",
        "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "            train_op = optimization.create_optimizer(\n",
        "              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "            # Calculate evaluation metrics. \n",
        "            def metric_fn(label_ids, predicted_labels):\n",
        "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "                true_pos = tf.metrics.true_positives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "                true_neg = tf.metrics.true_negatives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)   \n",
        "                false_pos = tf.metrics.false_positives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)  \n",
        "                false_neg = tf.metrics.false_negatives(\n",
        "                    label_ids,\n",
        "                    predicted_labels)\n",
        "\n",
        "                return {\n",
        "                    \"eval_accuracy\": accuracy,\n",
        "                    \"true_positives\": true_pos,\n",
        "                    \"true_negatives\": true_neg,\n",
        "                    \"false_positives\": false_pos,\n",
        "                    \"false_negatives\": false_neg,\n",
        "                    }\n",
        "\n",
        "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                  loss=loss,\n",
        "                  train_op=train_op)\n",
        "            else:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                    loss=loss,\n",
        "                    eval_metric_ops=eval_metrics)\n",
        "        else:\n",
        "            (predicted_labels, log_probs, output_layer) = create_model(\n",
        "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "            predictions = {\n",
        "              'probabilities': log_probs,\n",
        "              'labels': predicted_labels,\n",
        "              'pooled_output': output_layer\n",
        "            }\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    # Return the actual model function in the closure\n",
        "    return model_fn"
      ],
      "metadata": {
        "id": "PP1wElNM1w1w"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 1.0\n",
        "# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 300\n",
        "SAVE_SUMMARY_STEPS = 100\n",
        "\n",
        "# Compute train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Specify output directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "# Specify output directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "metadata": {
        "id": "wb-NSnDI10lQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_steps, len(label_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWr_IdvG17hI",
        "outputId": "0d731677-1ae1-48c1-9248-55703eea0a35"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFhAJEwc2A4P",
        "outputId": "a3662236-7e63-4d45-e602-0807f6723924"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/bert_news_category', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11703f06d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/bert_news_category', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11703f06d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)\n",
        "\n",
        "# Create an input function for validating. drop_remainder = True for using TPUs.\n",
        "val_input_fn = run_classifier.input_fn_builder(\n",
        "    features=val_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "metadata": {
        "id": "fBRVu4Vw2DBo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkykPJwA2I2G",
        "outputId": "59cdfb4f-a883-4a7b-a830-1c3e7a1948cc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-41-d311bc1ecf48>:35: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-41-d311bc1ecf48>:35: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /bert_news_category/model.ckpt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /bert_news_category/model.ckpt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:loss = 1.583061, step = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:loss = 1.583061, step = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 25 into /bert_news_category/model.ckpt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 25 into /bert_news_category/model.ckpt.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.8458912.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.8458912.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took time  0:01:28.266498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model with Validation set\n",
        "estimator.evaluate(input_fn=val_input_fn, steps=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FKUH6G42t2n",
        "outputId": "26955a8e-ef9f-48fd-cb0d-3a84e48bc0ff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2021-12-14T16:51:24Z\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2021-12-14T16:51:24Z\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /bert_news_category/model.ckpt-25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Restoring parameters from /bert_news_category/model.ckpt-25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2021-12-14-16:51:34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2021-12-14-16:51:34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saving dict for global step 25: eval_accuracy = 0.93, false_negatives = 3.0, false_positives = 1.0, global_step = 25, loss = 0.85419935, true_negatives = 27.0, true_positives = 69.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saving dict for global step 25: eval_accuracy = 0.93, false_negatives = 3.0, false_positives = 1.0, global_step = 25, loss = 0.85419935, true_negatives = 27.0, true_positives = 69.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25: /bert_news_category/model.ckpt-25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25: /bert_news_category/model.ckpt-25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.93,\n",
              " 'false_negatives': 3.0,\n",
              " 'false_positives': 1.0,\n",
              " 'global_step': 25,\n",
              " 'loss': 0.85419935,\n",
              " 'true_negatives': 27.0,\n",
              " 'true_positives': 69.0}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}